{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the GPS coordinates are valid\n",
    "import os\n",
    "geolife_dir = 'Geolife Trajectories 1.3/Data/'\n",
    "folder_list = os.listdir(geolife_dir)\n",
    "\n",
    "for folder in folder_list:  \n",
    "    trajectory_dir = geolife_dir + folder + '/Trajectory/'\n",
    "    user_trajectories = os.listdir(trajectory_dir)\n",
    "    trajectory_one_user = []\n",
    "    for plt in user_trajectories:\n",
    "        with open(trajectory_dir + plt, 'r', newline='', encoding='utf-8') as f:\n",
    "            GPS_logs = filter(lambda x: len(x.split(',')) == 7, f)\n",
    "            GPS_logs_split = map(lambda x: x.rstrip('\\r\\n').split(','), GPS_logs)\n",
    "            for row in GPS_logs_split:\n",
    "                if float(row[0])< -90 or float(row[0])>90 or float(row[1])< -180 or float(row[1])>180:\n",
    "                    print(f\"{folder}/{plt},{row[0],row[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\谢嘉楠\\AppData\\Local\\Temp\\ipykernel_21492\\863449168.py:50: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data['Datetime'] = pd.to_datetime(data['Date'] + ' ' + data['Time'])\n"
     ]
    }
   ],
   "source": [
    "# preprocessing step for the geolife dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    " \n",
    "data_dir = 'Geolife Trajectories 1.3/Data/'\n",
    "\n",
    "\n",
    "dirlist = os.listdir(data_dir)\n",
    "#list all the folders with labels.txt\n",
    "label_dirs=[]\n",
    "folder_dirs = []\n",
    "for dir in dirlist:  \n",
    "  if os.path.exists(data_dir + '/' +dir+'/labels.txt'):\n",
    "    label_dirs.append(data_dir + '/' + dir+'/'+'labels.txt')\n",
    "    folder_dirs.append(data_dir + '/' + dir+'/'+'Trajectory')\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    dlat = np.radians(lat2 - lat1)\n",
    "    dlon = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon/2) * np.sin(dlon/2)\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "traj=pd.DataFrame()\n",
    "\n",
    "num=0\n",
    "BEIJING = [39.9, 116.41]                                                    \n",
    "# central beijing coords, for map centres\n",
    "B1 = 39.8,116.2                                          \n",
    "# bbox limits for beijing extent\n",
    "B2 =  40.0 ,116.5\n",
    "\n",
    "for no,dir in enumerate(folder_dirs):\n",
    "    label_path = label_dirs[no]\n",
    "    label = pd.read_csv(label_path, sep='\\t', header=0)\n",
    "    label['Start Time'] = pd.to_datetime(label['Start Time'])\n",
    "    label['End Time'] = pd.to_datetime(label['End Time'])\n",
    "    for file in os.listdir(dir):\n",
    "        file_path = os.path.join(dir, file)\n",
    "        #read data:\n",
    "        data = pd.read_csv(file_path,\n",
    "                       header=None, \n",
    "                       skiprows=6,\n",
    "                       names=['Latitude', 'Longitude', 'Not_Important1', 'Altitude', 'Not_Important2', 'Date', 'Time'])\n",
    "        '''\n",
    "        merge date and time\n",
    "        '''\n",
    "        data['Datetime'] = pd.to_datetime(data['Date'] + ' ' + data['Time'])\n",
    "        data=data[['Latitude', 'Longitude', 'Altitude', 'Datetime']]\n",
    "\n",
    "        '''\n",
    "        merge label and data\n",
    "        '''\n",
    "        for i in range(len(label)):\n",
    "            mask=(data['Datetime']>=label.iloc[i,0]) & (data['Datetime']<=label.iloc[i,1])\n",
    "            data.loc[mask,'mode']=label.iloc[i,2] \n",
    "        '''\n",
    "        retain positions in Beijing city\n",
    "        '''\n",
    "        #data=data[(data['Latitude']>B1[0]) & (data['Latitude']<B2[0]) & (data['Longitude']>B1[1]) & (data['Longitude']<B2[1])] \n",
    "    \n",
    "        '''\n",
    "        time gap to 1s, and remain first record every 5s\n",
    "        '''\n",
    "        data['Datetime_1s']=data['Datetime'].dt.floor('1s')\n",
    "        data=data.drop_duplicates(subset=['Datetime_1s'],keep='first')\n",
    "    \n",
    "        '''\n",
    "        remove stopping point\n",
    "        '''\n",
    "        data['is_moving'] = (data['Latitude'] != data['Latitude'].shift()) | (data['Longitude'] != data['Longitude'].shift())\n",
    "        data=data[data['is_moving']==True]\n",
    "        data=data[['Latitude','Longitude','Datetime_1s','mode']]\n",
    "    \n",
    "    \n",
    "        '''\n",
    "        split trajs without records in 10min into 2 trajs （and update id）\n",
    "        '''\n",
    "        data['time_diff']=data['Datetime_1s'].diff()\n",
    "        data['split_id']=0\n",
    "        mask=data['time_diff']>pd.Timedelta(minutes=10)\n",
    "        data.loc[mask,'split_id']=1\n",
    "        data['split_id']=data['split_id'].cumsum()\n",
    "    \n",
    "\n",
    "        data['id']=str(num)\n",
    "        num+=1\n",
    "        data['id']=data['id']+'_'+data['split_id'].astype(str)\n",
    "    \n",
    "    \n",
    "        '''\n",
    "        calc each traj's length, filter out short trajs and truncate long ones\n",
    "        '''\n",
    "        #calculate nearby location's lon and lat gap\n",
    "        lat_lon_diff = data.groupby('id',group_keys=False).apply(lambda group: group[['Latitude', 'Longitude']].diff())\n",
    "        #calc nearby locationn's distance\n",
    "        distance = lat_lon_diff.apply(lambda row: haversine_distance(row['Latitude'], row['Longitude'], 0, 0), axis=1)\n",
    "        data['distance']=distance\n",
    "        #calculate each id's accumulated distance\n",
    "        data['accum_dis']=data.groupby('id')['distance'].cumsum()\n",
    "        #split those trajs longer than 10km into 2 trajs\n",
    "        data['split_traj_id']=data['accum_dis']//10\n",
    "        data['split_traj_id']=data['split_traj_id'].fillna(0)\n",
    "        data['split_traj_id']=data['split_traj_id'].astype(int).astype(str)\n",
    "        #get new id\n",
    "        data['id']=data['id']+'_'+data['split_traj_id']\n",
    "        #remove those shorter than 1km\n",
    "        iid=data.groupby('id')['accum_dis'].max()\n",
    "        iid=iid.reset_index(name='distance')\n",
    "        iid=iid[iid['distance']>1]\n",
    "        data=data[data['id'].isin(iid['id'])]\n",
    "    \n",
    "    \n",
    "        '''\n",
    "        filter trajs shorter than 10  records\n",
    "        '''\n",
    "        iid=data.groupby('id').size()\n",
    "        iid=iid.reset_index(name='count')\n",
    "        iid=iid[iid['count']>=10]\n",
    "        data=data[data['id'].isin(iid['id'])]\n",
    "    \n",
    "        '''\n",
    "        remove stay points\n",
    "        '''\n",
    "        latlon=pd.DataFrame()\n",
    "        latlon['max_lat']=data.groupby('id')['Latitude'].max()\n",
    "        latlon['min_lat']=data.groupby('id')['Latitude'].min()\n",
    "        latlon['max_lon']=data.groupby('id')['Longitude'].max()\n",
    "        latlon['min_lon']=data.groupby('id')['Longitude'].min()\n",
    "        latlon['max_dis']=latlon.apply(lambda row: haversine_distance(row['max_lat'],row['max_lon'],row['min_lat'],row['min_lon']),axis=1)\n",
    "        latlon=latlon[latlon['max_dis']>=1]\n",
    "    \n",
    "    \n",
    "        data=data[data['id'].isin(latlon.index)]\n",
    "        data=data[['Latitude','Longitude','Datetime_1s','id','mode']]\n",
    "\n",
    "        traj=pd.concat([traj,data])\n",
    "\n",
    "#merge and select the final modes ['walk', 'bike', 'bus', 'drive', 'train']\n",
    "traj.loc[traj['mode'].isin(['car', 'taxi']), 'mode'] = 'drive'\n",
    "traj.loc[traj['mode'].isin(['train', 'subway']), 'mode'] = 'train'\n",
    "traj = traj[traj['mode'].isin(['walk', 'bike', 'bus', 'drive', 'train'])]\n",
    "traj['mode'].replace({'walk':0,'bike':1,'bus':2,'drive':3,'train':4},inplace=True)\n",
    "\n",
    "#show the distribution of the modes at point level\n",
    "print(traj['mode'].value_counts())\n",
    "\n",
    "traj.to_csv('geolife_processed_full.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into segments with parameter M\n",
    "import pandas as pd\n",
    "M = 32\n",
    "df = pd.read_csv('geolife_processed_full.csv')\n",
    "\n",
    "df['segment_id'] = (df.groupby(['id', 'mode']).cumcount() // M).astype(int)\n",
    "df['id'] = df['id'] + '_' + df['segment_id'].astype(str)   \n",
    "\n",
    "#show the distribution of the modes at segment level\n",
    "print(df.groupby('mode')['id'].nunique())\n",
    "\n",
    "df.to_csv('geolife_processed_full_truncated{}.csv'.format(M),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain three trajectories with the most modes for visiualization\n",
    "import pandas as pd \n",
    "df = pd.read_csv('geolife_processed_full.csv')\n",
    "top_three_ids = df.groupby('id')['mode'].nunique().nlargest(3).index.tolist()\n",
    "for id in top_three_ids:\n",
    "    df_id = df[df['id']==id]\n",
    "    df_id.to_csv(f'trajectory to show_{id}.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
